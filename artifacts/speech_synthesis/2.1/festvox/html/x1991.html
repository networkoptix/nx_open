<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/loose.dtd">
<HTML
><HEAD
><TITLE
>Prosody Walkthrough</TITLE
><META
NAME="GENERATOR"
CONTENT="Modular DocBook HTML Stylesheet Version 1.79"><LINK
REL="HOME"
TITLE="Building Synthetic Voices"
HREF="book1.html"><LINK
REL="UP"
TITLE="Building prosodic models"
HREF="c1637.html"><LINK
REL="PREVIOUS"
TITLE="Prosody Research"
HREF="x1973.html"><LINK
REL="NEXT"
TITLE="Corpus development"
HREF="c2174.html"></HEAD
><BODY
CLASS="SECT1"
BGCOLOR="#FFFFFF"
TEXT="#000000"
LINK="#0000FF"
VLINK="#840084"
ALINK="#0000FF"
><DIV
CLASS="NAVHEADER"
><TABLE
SUMMARY="Header navigation table"
WIDTH="100%"
BORDER="0"
CELLPADDING="0"
CELLSPACING="0"
><TR
><TH
COLSPAN="3"
ALIGN="center"
>Building Synthetic Voices</TH
></TR
><TR
><TD
WIDTH="10%"
ALIGN="left"
VALIGN="bottom"
><A
HREF="x1973.html"
ACCESSKEY="P"
>&#60;&#60;&#60; Previous</A
></TD
><TD
WIDTH="80%"
ALIGN="center"
VALIGN="bottom"
>Building prosodic models</TD
><TD
WIDTH="10%"
ALIGN="right"
VALIGN="bottom"
><A
HREF="c2174.html"
ACCESSKEY="N"
>Next &#62;&#62;&#62;</A
></TD
></TR
></TABLE
><HR
ALIGN="LEFT"
WIDTH="100%"></DIV
><DIV
CLASS="SECT1"
><H1
CLASS="SECT1"
><A
NAME="AEN1991"
>Prosody Walkthrough</A
></H1
><P
>This section gives a walkthrough of a set of basic scripts that can be 
used to build duration and F0 models. The results will be reasonable 
but they are designed to be language independent and hence more 
appropriate models will almost certainly give better results. We have 
used these methods when building diphone voices for new languages when 
we know almost nothing explicit about the language structure. This 
walkthrough however explcitly covers most of the major steps and hence 
will be useful as a basis for building new better models. </P
><P
>In many ways this process is simialr to the limited domain voice 
building process. here we will design a set of prompts which are 
believed to cover the prosody that we wish to model, we record and label 
the data and then build models from the utterances built from the 
natural speech. In fact the basic structure for this uses the 
limited domain scripts for the initial part of the process. </P
><P
>The basic stages of this task are 
<P
></P
><UL
><LI
STYLE="list-style-type: disc"
><P
>Design database</P
></LI
><LI
STYLE="list-style-type: disc"
><P
>Setup directory structure</P
></LI
><LI
STYLE="list-style-type: disc"
><P
>Synthesizing prompts (for labeling)</P
></LI
><LI
STYLE="list-style-type: disc"
><P
>Recording prompts</P
></LI
><LI
STYLE="list-style-type: disc"
><P
>Phonetically label prompts</P
></LI
><LI
STYLE="list-style-type: disc"
><P
>Extract pitchmarks and F0 contour</P
></LI
><LI
STYLE="list-style-type: disc"
><P
>Build utterance structures</P
></LI
><LI
STYLE="list-style-type: disc"
><P
>For duration models
<P
></P
><UL
><LI
STYLE="list-style-type: disc"
><P
>   extract means and standard deviations of phone durations
   </P
></LI
><LI
STYLE="list-style-type: disc"
><P
>   extract features for prediction
   </P
></LI
><LI
STYLE="list-style-type: disc"
><P
>   build feature description files
   </P
></LI
><LI
STYLE="list-style-type: disc"
><P
>   Build regression model to predict durations
   </P
></LI
><LI
STYLE="list-style-type: disc"
><P
>   construct scheme file with duration model
   </P
></LI
></UL
></P
></LI
><LI
STYLE="list-style-type: disc"
><P
>For F0 models
<P
></P
><UL
><LI
STYLE="list-style-type: disc"
><P
>   extract features for prediction
   </P
></LI
><LI
STYLE="list-style-type: disc"
><P
>   build feature description files
   </P
></LI
><LI
STYLE="list-style-type: disc"
><P
>   Build regression model to preict F0 at start, mid and end of syllable
   </P
></LI
><LI
STYLE="list-style-type: disc"
><P
>   construct scheme file with F0 model
   </P
></LI
></UL
></P
></LI
></UL
></P
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="AEN2035"
>Design database</A
></H2
><P
>The object here is to cpature enough speech in prosodic style that you 
wish your syntehsizer to use. Note as prosodic modeling is still and 
extremely difficult area all models are extremely imporerished 
(especially the very simple models we are presenting here), thus do not 
be too ambitious. However it is worthwhile consider if you wish dialog 
(i.e. conversational speech) or prose (i.e. read speech). Prose can be 
news reader style or story telling style. Most synthesizers are trained 
on news reader style becuase its fairly consistent and believe to be 
easier to model, and reading paragraphs of text is seens as a basic 
apllication for text to speech synthesizers. However today with more 
dialog systems such prosodic models are often not as appropriate. </P
><P
>Ideally your database will be marked up with prosodic tagging that 
your voice talent will understand and be able to deliver appropriately. 
Designing such a database isn't easy but when starting off in new 
languages anything may be better than fixed durations and a naive 
declining F0. Thus simply a list of 500 sentences from newspapers 
may give rise to better models than. </P
><P
>Suppose you have your 500 sentences, construct a prompt list 
as is done with the limited domain constuction. That is, you 
need a file of the form. 
<A
NAME="AEN2040"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
CLASS="LITERALLAYOUT"
>(&nbsp;sent_0001&nbsp;"She&nbsp;had&nbsp;your&nbsp;dark&nbsp;suit&nbsp;in&nbsp;greasy&nbsp;washwater&nbsp;all&nbsp;year.")<br>
(&nbsp;sent_0002&nbsp;"Don't&nbsp;make&nbsp;me&nbsp;carry&nbsp;an&nbsp;oily&nbsp;rag&nbsp;like&nbsp;that.")<br>
(&nbsp;sent_0003&nbsp;"They&nbsp;wanted&nbsp;to&nbsp;go&nbsp;on&nbsp;a&nbsp;barge&nbsp;trip.")<br>
...</P
></BLOCKQUOTE
></P
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="AEN2042"
>Setup directory structure</A
></H2
><P
>As with the rest of the festvox tools, you need to set 
the following to environment variables to allow them to work 
properly. In <CODE
CLASS="VARNAME"
>bash</CODE
> or other Bourne shell compatibles 
type, with the appropriate pathnames for you installation of 
the Edinburgh Speech Tools an Festvox itself. 
<A
NAME="AEN2046"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
CLASS="LITERALLAYOUT"
>export&nbsp;FESTVOXDIR=/home/awb/projects/festvox<br>
export&nbsp;ESTDIR=/home/awb/projects/1.4.1/speech_tools</P
></BLOCKQUOTE
>
For <CODE
CLASS="VARNAME"
>csh</CODE
> and its derivative you should type 
<A
NAME="AEN2049"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
CLASS="LITERALLAYOUT"
>setenv&nbsp;FESTVOXDIR&nbsp;/home/awb/projects/festvox<br>
setenv&nbsp;ESTDIR&nbsp;/home/awb/projects/1.4.1/speech_tools</P
></BLOCKQUOTE
>
As the basic structure is so similar to the limited 
domain building structure, first you should all that 
setup procedure. If you are building prosodic models for 
an already existing limited domain then you do not 
need this part. 
<A
NAME="AEN2051"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
CLASS="LITERALLAYOUT"
>mkdir&nbsp;cmu_timit_awb<br>
cd&nbsp;cmu_timit_awb<br>
$FESTVOXDIR/src/ldom/setup_ldom&nbsp;cmu&nbsp;timit&nbsp;awb</P
></BLOCKQUOTE
>
The arguments are, institution, domain type, and speaker name. </P
><P
>After setting this up you need to also setup the extra 
directories and scripts need to build prosody models. This 
is done by the command 
<A
NAME="AEN2054"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
CLASS="LITERALLAYOUT"
>$FESTVOXDIR/src/prosody/setup_prosody</P
></BLOCKQUOTE
></P
><P
>You shold copy your database files as created in the previous section 
into <TT
CLASS="FILENAME"
>etc/</TT
>. </P
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="AEN2058"
>Synthesizing prompts</A
></H2
><P
>We then synthesizer the prompts. As we are trying to collect natural 
speech these prompts should not normally be presented to the voice 
talent as they may then copy the syntehsizer intonation, which would 
almost certainly be a bad thing. As this will sometimes be the first 
serious use of a new diphone syntehsizerin a new language, (with 
impoverished prosody models) it is important to check that the prompts 
can be generate phonetically correct. This may require more additions 
to the lexicon and/or more token to word rules. We synthesize the 
prompts for two reasons. First, to use for autolabeling in that the 
synthesized prompts will be aligned using dtw against what the speaker 
actually says. Second we are trying to construct festival utterances 
structures for each utterance in this database with natural durations 
and F0. so we may learn from them. </P
><P
>You should change the line setting the "closest" voice 
<A
NAME="AEN2062"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
CLASS="LITERALLAYOUT"
>(set!&nbsp;cmu_timit_awb::closest_voice&nbsp;'voice_kal_diphone)</P
></BLOCKQUOTE
>
This is in the file <TT
CLASS="FILENAME"
>festvox/cmu_timit_awb_ldom.scm</TT
>. This is the 
voice that will be used to syntehsized the prompts. Often this will be 
your new diphone voice. </P
><P
>Ideally we would like these utterances to also have natural phone 
sequences, such that schwas, allophones such as flaps, and post-lexical 
rules have been applied. At present we do not include that here though 
for more serious prosody modeling such phonomena should be included in 
the utterance structures here. </P
><P
>The prompts can be synthesizer by the command 
<A
NAME="AEN2067"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
CLASS="LITERALLAYOUT"
>festival&nbsp;-b&nbsp;festvox/build_ldom.scm&nbsp;'(build_prompts&nbsp;"etc/timit.data")'</P
></BLOCKQUOTE
></P
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="AEN2069"
>Recording the prompts</A
></H2
><P
>The usual caveats apply to recording, (<A
HREF="x817.html"
>the Section called <I
>Recording under Unix</I
> in the Chapter called <I
>Basic Requirements</I
></A
>) and the issues on selecting a speaker.</P
><P
>As prosody modeling is difficult, and if you are inexperienced in 
building such models, it is wise not to attempt anything hard. Just 
building reliable models for default unmarked intonation is very useful 
if your current models are simply the default fixed intonation. Thus 
the senetences should be read in a natural but not too varied style. </P
><P
>Recording can be done with <CODE
CLASS="VARNAME"
>pointyclicky</CODE
> or <CODE
CLASS="VARNAME"
>prompt_them</CODE
>. 
If you are using <CODE
CLASS="VARNAME"
>prompt_them</CODE
>, you should modify that script so 
that it does not play the prompts, as they will confuse the speaker. 
The speaker should simply read the text (and markup, if present). 
<A
NAME="AEN2078"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
CLASS="LITERALLAYOUT"
>pointyclicky&nbsp;etc/timit.data</P
></BLOCKQUOTE
>
or 
<A
NAME="AEN2080"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
CLASS="LITERALLAYOUT"
>bin/prompt_them&nbsp;etc/timit.data</P
></BLOCKQUOTE
></P
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="AEN2082"
>Phonetically label prompts</A
></H2
><P
>After recording the spoken utterances must be labeled 
<A
NAME="AEN2085"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
CLASS="LITERALLAYOUT"
>bin/make_labs&nbsp;prompt-wav/*.wav</P
></BLOCKQUOTE
>
This is one of the computationally expensive parts of the process and 
for longer sentences it can require much memory too. </P
><P
>After autolabeling it is always worthwhiel to inspect the labels and 
correct mistakes. Phrasing can particularly cause problems so adding or 
deleting silences can make the derived prosody models much more 
accurate. You can use emulabel to to this. 
<A
NAME="AEN2088"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
CLASS="LITERALLAYOUT"
>emulabel&nbsp;etc/emu_lab</P
></BLOCKQUOTE
></P
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="AEN2090"
>Extract pitchmarks and F0</A
></H2
><P
>At this point we diverge from the process used for building limited 
domain synthesizers. You can construct such synthesizers from the same 
recordings, maybe you wish more appropriate prosodic models for the 
fallback synthesizer. But at this poijnt we need to extract the 
pitchmark in a slightl different way. We are intending to extract F0 
contours for all non-silence parts of the speech signal. We do 
this by extracting pitchmarks for the voiced sections alone 
then (in the next section) interpolating the F0 through the non-voiced 
(but non-silence) sections. </P
><P
><A
HREF="x862.html"
>the Section called <I
>Extracting pitchmarks from waveforms</I
> in the Chapter called <I
>Basic Requirements</I
></A
>
discusses the setting of
parameters to get <TT
CLASS="FILENAME"
>bin/make_pm_wave</TT
> to work for a particular 
voice. In this case we need those same parameters (which should be 
found by experiment). These shold be copied from 
<TT
CLASS="FILENAME"
>bin/make_pm_wave</TT
> and added to <TT
CLASS="FILENAME"
>bin/make_F0_pm</TT
> in the 
variable <CODE
CLASS="VARNAME"
>PM_ARGS</CODE
>. The distribution contains something 
like 
<A
NAME="AEN2099"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
CLASS="LITERALLAYOUT"
>PM_ARGS='-min&nbsp;0.0057&nbsp;-max&nbsp;0.012&nbsp;-def&nbsp;0.01&nbsp;-wave_end&nbsp;-lx_lf&nbsp;140&nbsp;-lx_lo&nbsp;111&nbsp;-lx_hf&nbsp;80&nbsp;-lx_ho&nbsp;51&nbsp;-med_o&nbsp;0'</P
></BLOCKQUOTE
>
Importnantly this differs from the parameters in <CODE
CLASS="VARNAME"
>bin/make_pm_wave</CODE
> 
as we do not use the <CODE
CLASS="VARNAME"
>-fill</CODE
> option to fill in
pitchmarks over the rest of the waveform.</P
><P
>The second part of this section is the construction of an F0 contour 
which is build from the extracted pitchmarks. Unvoiced speech sections 
are assigned an F0 contour by interpolation from the voiced section 
around it, and the result is smnoothed. The label files are used to 
define which parts of the signal are silence and which are speech. </P
><P
>The variable <CODE
CLASS="VARNAME"
>SILENCE</CODE
> in <CODE
CLASS="VARNAME"
>bin/make_f0_pm</CODE
> must be modified to 
reflect the symbol used for silence in your phoneset. </P
><P
>Once the pitchmark parameters have be determined, and the appropriate 
<CODE
CLASS="VARNAME"
>SILENCE</CODE
> value set you can extract the smoothed F0 by the command 
<A
NAME="AEN2109"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
CLASS="LITERALLAYOUT"
>bin/make_f0_pm&nbsp;wav/*.wav</P
></BLOCKQUOTE
></P
><P
>You can view the F0 contrours with the command 
<A
NAME="AEN2112"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
CLASS="LITERALLAYOUT"
>emulabel&nbsp;etc/emu_f0</P
></BLOCKQUOTE
></P
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="AEN2114"
>Build utterance structures</A
></H2
><P
>With the labels and F0 created we can now rebuild the utterance 
structures by syntehsizing the prompt snad merging in the values from 
the natural durations and F0 from the naturally spoken utterances. 
<A
NAME="AEN2117"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
CLASS="LITERALLAYOUT"
>festival&nbsp;-b&nbsp;festvox/build_ldom.scm&nbsp;'(build_utts&nbsp;"etc/timit.data")'</P
></BLOCKQUOTE
></P
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="AEN2119"
>Duration models</A
></H2
><P
>The script <CODE
CLASS="VARNAME"
>bin/make_dur_model</CODE
> contains all of 
the following commands but it is wise to understand the stages as 
due to errors in labeling it may not all run completely smoothly 
and small fixes may be required. </P
><P
>We are building a duration model using a CART tree to predict zscore 
values for phones. Zscores (number of standard deviations from the 
mean) have often been used in duration modeling as they allow a certain 
amount of normalization over different phones. </P
><P
>You shold first look at the script <CODE
CLASS="VARNAME"
>bin/make_dur_model</CODE
> and 
edit the following three variable values 
<A
NAME="AEN2126"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
CLASS="LITERALLAYOUT"
>SILENCENAME=SIL<br>
VOICENAME='(kal_diphone)'<br>
MODELNAME=cmu_us_kal</P
></BLOCKQUOTE
>
these should contain the name for silence in your phoneset, the call for 
the voice you are building the model for (or at least one that uses the 
same phoneset), and finally the name for the model, which can be the 
same <CODE
CLASS="VARNAME"
>INST_LANG_VOX</CODE
> part of the voice you call. </P
><P
>The first stage is to find the means and standard deviations for 
each phone. A festival script in the festival distribution 
is used to load in all the utetrances and a calculate these 
values. With the command 
<A
NAME="AEN2130"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
CLASS="LITERALLAYOUT"
>durmeanstd&nbsp;-output&nbsp;festival/dur/etc/durs.meanstd&nbsp;festival/utts/*.utt</P
></BLOCKQUOTE
>
You should check <TT
CLASS="FILENAME"
>festival/dur/etc/durs.meanstd</TT
>, the generated 
file to ensure that the numbers look raosnable. If there is only one 
example of a particular phone, the standard deviation cannot be 
calculated and the value is given as <CODE
CLASS="VARNAME"
>nan</CODE
> (not-a-number). Thus 
must be changed to a standard numeric value (say one-third or the mean). 
Also some of the values in this table maybe adversely affected by bad 
labeling so you may wish to hand modify the values, or go back and 
correct the labeling. </P
><P
>The next stage is extract the features from which we will predict the 
durations. The list of features extracted as in 
<TT
CLASS="FILENAME"
>festival/dur/etc/dur/feats</TT
>. These cover phonetic context, 
syllable, word position etc. These may or may not be appropriate for 
your new language or domain and you you may wish to add to these before 
doing the extraction. The extraction process takes each phoneme and 
dumps the named feature values for that phone into a file. This uses 
the standard festival script <CODE
CLASS="VARNAME"
>dumpfeats</CODE
> to do this. The command 
looks like 
<A
NAME="AEN2137"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
CLASS="LITERALLAYOUT"
>$DUMPFEATS&nbsp;-relation&nbsp;Segment&nbsp;-eval&nbsp;$VOICENAME&nbsp;\<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-feats&nbsp;festival/dur/etc/dur.feats&nbsp;=<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-output&nbsp;festival/dur/feats/%s.feats&nbsp;\<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-eval&nbsp;festival/dur/etc/logdurn.scm&nbsp;\<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;festival/utts/*.utt</P
></BLOCKQUOTE
>
These feature files are then concatenated into a single file which is 
then split (90/10) into traing and test sets. The training set is 
further split force use as a held-out testset used in the training 
phase. Also at this stage we remove all silence phones form the 
training and test set. This is, perhaps naively, because the 
distribution of silences is very wide and often files contain silences 
at the start and end of utterances which themslves aren't part of the 
speech content (they're just the edges) and having these in the training 
set can skew the results. </P
><P
>This is done by the commands 
<A
NAME="AEN2140"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
CLASS="LITERALLAYOUT"
>cat&nbsp;festival/dur/feats/*.feats&nbsp;|&nbsp;\<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;awk&nbsp;'{if&nbsp;($2&nbsp;!=&nbsp;"'$SILENCENAME'")&nbsp;print&nbsp;$0}'&nbsp;&#62;festival/dur/data/dur.data<br>
bin/traintest&nbsp;festival/dur/data/dur.data<br>
bin/traintest&nbsp;festival/dur/data/dur.data.train</P
></BLOCKQUOTE
></P
><P
>For <CODE
CLASS="VARNAME"
>wagon</CODE
> the CART tree builder to work it needs to know what 
possible values each feature can take. This can mostly be determined 
automatically but some features may have values that could be either 
numeric or classes, thus we use a post-processing function on the 
automatically generated description file to get our desired result. 
<A
NAME="AEN2144"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
CLASS="LITERALLAYOUT"
>$ESTDIR/bin/make_wagon_desc&nbsp;festival/dur/data/dur.data&nbsp;\<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;festival/dur/etc/dur.feats&nbsp;festival/dur/etc/dur.desc<br>
festival&nbsp;-b&nbsp;--heap&nbsp;2000000&nbsp;festvox/build_prosody.scm&nbsp;\<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$VOICENAME&nbsp;'(build_dur_feats_desc)'</P
></BLOCKQUOTE
></P
><P
>Now we can build the model itself. A key factor in the time this takes 
(and the accuracy of the model) is the "stop" value, that is the number 
of examples that must exist before a split searched for. The smaller 
this number the longer the search will be, though up to a certasin point 
the more accurate the model will be. But at some level this will over 
train. The default in the distribution is 50 which may or may not be 
appropriate. Not for large databases and for smaller values of 
<CODE
CLASS="VARNAME"
>STOP</CODE
> the training may take days even on a fast processor. </P
><P
>Although we have guessed a reasonable value for this for databases of 
around 50-1000 utterances it may not be appropriate for you. </P
><P
>The learning technique used is basic CART tree growing but with an 
important extention which makes the process much more robust on unseen 
data but unfortunately much more computationally expensive. The 
<CODE
CLASS="VARNAME"
>-stepwise</CODE
> option on wagon incrementally searches for the best 
features to use in building three, in addition to at each iteration 
finding the best questions about each feature that best model data. 
If you want a quicker result removing the <CODE
CLASS="VARNAME"
>-stepwise</CODE
> option 
will give you that. </P
><P
>The basic wagon command is 
<A
NAME="AEN2153"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
CLASS="LITERALLAYOUT"
>wagon&nbsp;-data&nbsp;festival/dur/data/dur.data.train.train&nbsp;\<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-desc&nbsp;festival/dur/etc/dur.desc&nbsp;\<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-test&nbsp;festival/dur/data/dur.data.train.test&nbsp;\<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-stop&nbsp;$STOP&nbsp;\<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-output&nbsp;festival/dur/tree/$PREF.S$STOP.tree&nbsp;\<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-stepwise&nbsp;</P
></BLOCKQUOTE
>
To tets the results on data not used inthe training we use 
the command 
<A
NAME="AEN2155"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
CLASS="LITERALLAYOUT"
>wagon_test&nbsp;-heap&nbsp;2000000&nbsp;-data&nbsp;festival/dur/data/dur.data.test&nbsp;\<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-desc&nbsp;festival/dur/etc/dur.desc&nbsp;\<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-tree&nbsp;festival/dur/tree/$PREF.S$STOP.tree</P
></BLOCKQUOTE
>
Interpreting the results isn't easy in isolation. The smaller the RMSE 
(root mean squared error) the better and the larger the correlation is 
the better (it should never be greater than 1, and should never be below 
0, though if you model is very bad it can be below 0). For English, 
with this script on a Timit database we get an RMSE value of 0.81 and 
correlation of 0.58, on the test data. Note these values are not in the 
abosolute domain (i.e. seconds) they are in the zscore domain. </P
><P
>The final stage, probabaly after a number of iterations of the build 
process we must package model into a scheme file that can be used with a 
voice. This scheme file contains the means and standard deviations (so 
we can convert the predicted values back into seconds) and the 
prediction tree itself. We also add in predictions for the silence 
phone by hand. The comamnd to generate this is 
<A
NAME="AEN2158"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
CLASS="LITERALLAYOUT"
>festival&nbsp;-b&nbsp;--heap&nbsp;2000000&nbsp;\<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;festvox/build_prosody.scm&nbsp;$VOICENAME&nbsp;\<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'(finalize_dur_model&nbsp;"'$MODELNAME'"&nbsp;"'$PREF.S$STOP.tree'")'</P
></BLOCKQUOTE
></P
><P
>This will generate a file <TT
CLASS="FILENAME"
>festvox/cmu_timit_awb_dur.scm</TT
>. If you 
model name is the same as the basic diphone voice you intend to use it 
in you cna simply copy this file to the <TT
CLASS="FILENAME"
>festvox/</TT
> directory of 
your diphone voice and it will automatically work. But it is worth 
explaining what this install process really is. The duration model 
scheme file contains two lisp expression setting the the variables 
<CODE
CLASS="VARNAME"
>MODELNAME::phone_durs</CODE
> and <CODE
CLASS="VARNAME"
>MODELNAME::zdurtree</CODE
>. To 
uses these in a voice you must load this file, typically by 
adding 
<A
NAME="AEN2165"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
CLASS="LITERALLAYOUT"
>(require&nbsp;'MODELNAME_dur)</P
></BLOCKQUOTE
>
to the diphone voice definition file 
(<TT
CLASS="FILENAME"
>festvox/MODELNAME_diphone.scm</TT
>). And then get the voice 
defintion to use these new variables. This is done by the 
commands in the voice definition function 
<A
NAME="AEN2168"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
CLASS="LITERALLAYOUT"
>&nbsp;&nbsp;;;&nbsp;Duration&nbsp;prediction<br>
&nbsp;&nbsp;(set!&nbsp;duration_cart_tree&nbsp;MODELNAME::zdurtree)<br>
&nbsp;&nbsp;(set!&nbsp;duration_ph_info&nbsp;MODELNAME::phone_durs)<br>
&nbsp;&nbsp;(Parameter.set&nbsp;'Duration_Method&nbsp;'Tree_ZScores)</P
></BLOCKQUOTE
></P
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="AEN2170"
>F0 contour models</A
></H2
><P
>(what about accents ?) </P
><P
>extract features for prediction 
build feature description files 
Build regression model to preict F0 at start, mid and end of syllable 
construct scheme file with F0 model </P
></DIV
></DIV
><DIV
CLASS="NAVFOOTER"
><HR
ALIGN="LEFT"
WIDTH="100%"><TABLE
SUMMARY="Footer navigation table"
WIDTH="100%"
BORDER="0"
CELLPADDING="0"
CELLSPACING="0"
><TR
><TD
WIDTH="33%"
ALIGN="left"
VALIGN="top"
><A
HREF="x1973.html"
ACCESSKEY="P"
>&#60;&#60;&#60; Previous</A
></TD
><TD
WIDTH="34%"
ALIGN="center"
VALIGN="top"
><A
HREF="book1.html"
ACCESSKEY="H"
>Home</A
></TD
><TD
WIDTH="33%"
ALIGN="right"
VALIGN="top"
><A
HREF="c2174.html"
ACCESSKEY="N"
>Next &#62;&#62;&#62;</A
></TD
></TR
><TR
><TD
WIDTH="33%"
ALIGN="left"
VALIGN="top"
>Prosody Research</TD
><TD
WIDTH="34%"
ALIGN="center"
VALIGN="top"
><A
HREF="c1637.html"
ACCESSKEY="U"
>Up</A
></TD
><TD
WIDTH="33%"
ALIGN="right"
VALIGN="top"
>Corpus development</TD
></TR
></TABLE
></DIV
></BODY
></HTML
>